{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Medium blog \"Deep Learning to Jump\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by Maxime Bergeron, Ivan Sergienko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best deep learning practices require, at a minimum, having a test set and learning in batches. \n",
    "The code implemented in function `learn()` does not adhere to best practices. It is only intended to illustrate the points made in our blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(31415)    # Random seed fixed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and plot jump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 1000    # number of training samples\n",
    "t = torch.FloatTensor(n, 1).uniform_(0, 1)\n",
    "y = torch.ones_like(t)\n",
    "drop_size, location = 0.3, 0.4\n",
    "y[t > location] = 1 - drop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first 100 points\n",
    "sns.set(style=\"darkgrid\", palette=\"muted\", color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"Set1\", n_colors=8, desat=.5))\n",
    "\n",
    "t_plot = t[:100].squeeze(dim=1)\n",
    "y_plot = y[:100].squeeze(dim=1)\n",
    "\n",
    "ax = sns.scatterplot(t_plot, y_plot, color='C1')\n",
    "plt.title(\"Jump Training Data\", fontsize=14)\n",
    "ax.set_xlabel(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch does not have a step activation function, so we define it\n",
    "def heaviside(x):\n",
    "    # 0.5 * (torch.sign(x) + 1) works too, but we want to be true to the mathematical\n",
    "    #     definition of Heaviside function, without the point in the middle at 0\n",
    "    \n",
    "    y = torch.ones_like(x)\n",
    "    y[x < 0] = 0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneNode(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    a simple 1-dimensional neural net with a single hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, act_func):\n",
    "        super(OneNode, self).__init__()\n",
    "\n",
    "        self.act_func = act_func\n",
    "        \n",
    "        # input and output linear layers\n",
    "        self.linears = torch.nn.ModuleList([torch.nn.Linear(1, 1) for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](self.act_func(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model, x, y, epoch, ax):\n",
    "    \"\"\"\n",
    "    Plot part of learning data and current model predictions\n",
    "    \"\"\"\n",
    "    x_data = x[:100].squeeze(dim=1)\n",
    "    y_data = y[:100].squeeze(dim=1)\n",
    "    \n",
    "    x_pred = torch.linspace(torch.min(x), torch.max(x),100).unsqueeze(dim=1)\n",
    "    y_pred = model(x_pred).squeeze(dim=1).detach().numpy()\n",
    "    x_pred = x_pred.squeeze(dim=1)\n",
    "\n",
    "    sns.scatterplot(x_data, y_data, color='C1', ax=ax)\n",
    "    ax.set_title(f\"Epoch {epoch}\", fontsize=14)\n",
    "    ax.set_xlabel(\"time\")\n",
    "    sns.lineplot(x_pred, y_pred, color='C1', ax=ax)\n",
    "\n",
    "def learn(model, x, y, lr=0.1):\n",
    "    \"\"\"\n",
    "    Use model to learn the function f(x)=y.\n",
    "    lr is Adam learning rate.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    n_epochs = 20000\n",
    "    loss_array = torch.zeros(n_epochs)\n",
    "    best_model, min_loss = model, 1\n",
    "\n",
    "    sns.set(style=\"darkgrid\", palette=\"muted\", color_codes=True)\n",
    "    sns.set_palette(sns.color_palette(\"Set1\", n_colors=8, desat=.5))\n",
    "    plt.suptitle(\"Learning progress\", weight='bold')\n",
    "    f, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    plot_epochs = [n_epochs // 4, n_epochs // 2, 3 * n_epochs // 4, n_epochs]\n",
    "    plot_epochs = [100, 1000, 3000, n_epochs]\n",
    "    plot_counter = 0\n",
    "\n",
    "    for e in range(n_epochs):\n",
    "        prediction = model(x)\n",
    "        loss = loss_func(prediction, y)\n",
    "\n",
    "        optimizer.zero_grad()  # clear gradients for next cycle\n",
    "        loss.backward()        # backpropagation, compute gradients\n",
    "        optimizer.step()       # step in the right direction\n",
    "        loss_array[e] = loss\n",
    "\n",
    "        if loss.item() < min_loss:  # keep track of best model\n",
    "            best_model = copy.deepcopy(model)\n",
    "            min_loss = loss.item()\n",
    "\n",
    "        if (e + 1) in plot_epochs:\n",
    "            plot(best_model, x, y, e+1, axes[plot_counter // 2, plot_counter % 2])\n",
    "            plot_counter += 1\n",
    "        \n",
    "    plt.show()\n",
    "    return loss_array, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_array, title=\"\"):\n",
    "    ax = sns.lineplot(x=torch.arange(len(loss_array)), y=loss_array.detach().numpy())\n",
    "    ax.set(yscale=\"log\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialise vanilla 1-dim neural net with single hidden layer\n",
    "model = OneNode(act_func=torch.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model on the step function\n",
    "loss_array, model = learn(model=model, x=t, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss over traing time\n",
    "plot_loss(loss_array, \"MSE for Sigmoid Unit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heaviside Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise vanilla 1-dim neural net with single hidden layer\n",
    "model = OneNode(act_func=heaviside)\n",
    "# train the model on the step function\n",
    "loss_array, model = learn(model=model, x=t, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss over training time\n",
    "plot_loss(loss_array, \"MSE for Heaviside Unit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Heaviside loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wH = -0.3\n",
    "bH = 1.0\n",
    "N = 20\n",
    "def MSE(b):\n",
    "    tmp = y[:N] - wH * heaviside(t[:N] + b) - bH\n",
    "    return torch.sum(tmp * tmp) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_in = np.linspace(-1, 0, 100)\n",
    "mse = np.array([MSE(b) for b in b_in])\n",
    "\n",
    "ax = sns.scatterplot(b_in, mse, color='C2', s=15)\n",
    "plt.title(\"MSE vs bias for Heaviside unit\", fontsize=14)\n",
    "ax.set_xlabel(r'$b_{in}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jump Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpUnit(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    a simple 1-dimensional network with a single hidden layer and Heaviside activation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(JumpUnit, self).__init__()\n",
    "\n",
    "        self.act_func = torch.sigmoid\n",
    "        \n",
    "        # On linear node for input and output\n",
    "        self.linears = torch.nn.ModuleList([torch.nn.Linear(1, 1) for _ in range(2)])\n",
    "        self.linears.append(torch.nn.Linear(1, 1, bias=False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linears[0](x)\n",
    "        h = self.linears[1](heaviside(x))\n",
    "        s = self.linears[2](self.act_func(x))\n",
    "        \n",
    "        return h + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize jump unit net\n",
    "model = JumpUnit()\n",
    "# train the model on the step function\n",
    "loss_array, model = learn(model=model, x=t, y=y, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss over training time\n",
    "plot_loss(loss_array, \"MSE for Jump Unit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Jump Unit loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wH = -0.3\n",
    "bH = 1.0\n",
    "w_in = 1.0\n",
    "N = 20\n",
    "def MSE_jump_unit(wS, b):\n",
    "    tmp = y[:N] - (wS * torch.sigmoid(w_in * t[:N] + b) + wH * heaviside(w_in * t[:N] + b) + bH)\n",
    "    return torch.sum(tmp * tmp) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_in = np.linspace(-1, 0, 50)\n",
    "w_s = np.linspace(-0.5, 0.5, 50)\n",
    "bs, ws = np.meshgrid(b_in, w_s)\n",
    "\n",
    "bs_1 = np.reshape(bs, -1)\n",
    "ws_1 = np.reshape(ws, -1)\n",
    "\n",
    "mse_ju = np.array([MSE_jump_unit(w, b) for w, b in zip(ws_1, bs_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,7])\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_trisurf(ws_1, bs_1, mse_ju, cmap=plt.cm.jet, linewidth=0.01)\n",
    "ax.view_init(azim=65, elev=50)\n",
    "plt.title(\"MSE for Jump Unit\")\n",
    "plt.ylabel(r\"$b_{in}$\")\n",
    "plt.xlabel(r\"$w_S$\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
